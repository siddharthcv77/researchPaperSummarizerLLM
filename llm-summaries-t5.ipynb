{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10070613,"sourceType":"datasetVersion","datasetId":6207101}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch langchain transformers torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-01T20:42:59.758232Z","iopub.execute_input":"2024-12-01T20:42:59.758576Z","iopub.status.idle":"2024-12-01T20:43:12.629820Z","shell.execute_reply.started":"2024-12-01T20:42:59.758544Z","shell.execute_reply":"2024-12-01T20:43:12.628912Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting langchain\n  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.9.2)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\nCollecting packaging>=20.0 (from transformers)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\nDownloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.9 langchain-core-0.3.21 langchain-text-splitters-0.3.2 langsmith-0.1.147 packaging-24.2 requests-toolbelt-1.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import necessary modules\nfrom transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T20:43:12.632006Z","iopub.execute_input":"2024-12-01T20:43:12.632444Z","iopub.status.idle":"2024-12-01T20:43:30.857737Z","shell.execute_reply.started":"2024-12-01T20:43:12.632397Z","shell.execute_reply":"2024-12-01T20:43:30.856735Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the summarization pipeline (default: BART)\ndef load_summarizer(model_name=\"facebook/bart-large-cnn\"):\n    \"\"\"\n    Load the summarization model pipeline.\n    Args:\n        model_name (str): The name of the Hugging Face model.\n    Returns:\n        summarizer function\n    \"\"\"\n    if model_name.startswith(\"t5\"):\n        # Use T5 summarizer\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        def t5_summarizer(text):\n            input_ids = tokenizer.encode(f\"summarize: {text}\", return_tensors=\"pt\", truncation=True, max_length=512)\n            outputs = model.generate(input_ids, max_length=130, min_length=30, length_penalty=2.0, num_beams=4)\n            return tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        return t5_summarizer\n    # else:\n    #     # Use BART summarizer\n    #     summarizer = pipeline(\"summarization\", model=model_name, device=0)\n        \n    #     def bart_summarizer(text):\n    #         summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n    #         return summary[0][\"summary_text\"]\n        \n    #     return summarizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T20:43:30.858853Z","iopub.execute_input":"2024-12-01T20:43:30.859398Z","iopub.status.idle":"2024-12-01T20:43:30.865331Z","shell.execute_reply.started":"2024-12-01T20:43:30.859356Z","shell.execute_reply":"2024-12-01T20:43:30.864446Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# Split text into manageable chunks using LangChain's RecursiveCharacterTextSplitter\ndef split_text_with_langchain(text, chunk_size=4096, chunk_overlap=200):\n    \"\"\"\n    Splits the text into manageable chunks using LangChain's RecursiveCharacterTextSplitter.\n    Args:\n        text (str): The text to split.\n        chunk_size (int): Maximum size of each chunk in tokens.\n        chunk_overlap (int): Number of overlapping characters between chunks.\n    Returns:\n        List of text chunks.\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n    )\n    chunks = text_splitter.split_text(text)\n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T20:43:30.867727Z","iopub.execute_input":"2024-12-01T20:43:30.868127Z","iopub.status.idle":"2024-12-01T20:43:30.884307Z","shell.execute_reply.started":"2024-12-01T20:43:30.868086Z","shell.execute_reply":"2024-12-01T20:43:30.883300Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Function to summarize text\ndef summarize_text(file, model_name=\"facebook/bart-large-cnn\"):\n    \"\"\"\n    Summarizes the content of a given .txt file.\n    Args:\n        file_path (str): Path to the .txt file.\n        model_name (str): Hugging Face model for summarization.\n    Returns:\n        Summary as a string.\n    \"\"\"\n    # Load summarization model\n    summarizer = load_summarizer(model_name)\n\n    # Read text from the file\n    with open(file, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n    \n    # Split text if it's too long\n    chunks = split_text_with_langchain(text, chunk_size=4096, chunk_overlap=200)\n\n    # Summarize each chunk\n    summaries = []\n    for chunk in chunks:\n        try:\n            summary = summarizer(chunk)\n            # print(\"Chunk: \", chunk, \" - - - - \", \"Summary: \", summary, \"\\n\\n\")\n            summaries.append(summary)\n        except Exception as e:\n            print(f\"Error summarizing chunk: {e}\")\n\n    # Combine all summaries\n    final_summary = \" \".join(summaries)\n    return final_summary\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T20:43:30.885361Z","iopub.execute_input":"2024-12-01T20:43:30.885739Z","iopub.status.idle":"2024-12-01T20:43:30.901398Z","shell.execute_reply.started":"2024-12-01T20:43:30.885709Z","shell.execute_reply":"2024-12-01T20:43:30.900480Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# Path to the .txt file in your environment\ninput_file = \"/kaggle/input/map-reduce-2/mapreduce_osdi04.txt\"\n\n# Choose the model (T5 or BART can be used here)\nmodel_name = \"t5-small\"  # Change to \"facebook/bart-large-cnn\" if you prefer BART\n\n# Summarize the text\ntry:\n    summary = summarize_text(input_file, model_name)\n    print(\"Summaries done\")\n    print(summary)\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T20:43:30.902529Z","iopub.execute_input":"2024-12-01T20:43:30.902936Z","iopub.status.idle":"2024-12-01T20:44:31.429391Z","shell.execute_reply.started":"2024-12-01T20:43:30.902896Z","shell.execute_reply":"2024-12-01T20:44:31.428364Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42ad48cef8d46b4838adca4d9fc74f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d900f957d114461b644d008c2386c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884deb27b079497191c6273db9483baa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce51634df844f06b3de15b68bc35ae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c451428db2749fcabfac1e706d35073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"214fa88e4e464ed99575933a9c85266e"}},"metadata":{}},{"name":"stdout","text":"Summaries done\nmap function processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. many real world tasks are expressible in this model, as shown in the paper. the run-time system takes care of the details of partitioning the input data, scheduling the pro- gram's execution across a set of machines, handling ma- chine failures, and managing the required inter-machine communication. the user of the MapReduce library expresses the computation as two functions: Map and Reduce. Map, written by the user, takes an input pair and pro- duces a set of intermediate key/value pairs. the reducefunction sums together all counts emitted for a particular word. duce function is passed all per-document term vectors for a given host. it adds these term vectors together, throwing away infrequent terms, and then emits a nal hhostname; term vectori pair. the reduce function accepts all pairs for a given word, sorts the corresponding document IDs and emits a hkey; recordi pair. it is easy to augment this computation to keep track of word positions. a worker who is assigned a map task reads the contents of the corresponding input split. the interme- diate key/value pairs produced by the Map function are buffered in memory. when a reduce worker is notied by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. the master keeps several data structures for each map task and reduce task, it stores the state (idle, in-progress, or completed map and reduce operators are de- terministic functions of their input values. each in-progress task writes its output to private temporary les. reduce worker atomically renames its temporary output le to the nal output le. if the same reduce task is executed on multi- ple machines, multiple rename calls will be executed for the same nal output le. if the same reduce task is executed on multi- ple machines, multiple rename calls will be executed for the same  3.5 Task Granularity We subdivide the map phase into M pieces and the re- duce phase into R pieces, as described above. Ideally, M and R should be much larger than the number of worker machines. the master must make O(M + R) scheduling decisions and keeps O(M R) state in memory as described above. 4.2 Ordering Guarantees We guarantee that within a given partition, the interme- diate key/value pairs are processed in increasing key or- der. 4.3 Combiner Function In some cases, there is signicant repetition in the inter- mediate keys produced by each map task. the user- specied Reduce function is commutative and associa-tive. mapReduce library detects which records cause deterministic crashes. each worker process installs a signal handler that catches segmentation violations and bus errors. when the user code generates a signal, the signal handler sends a last gasp UDP packet that contains the sequence number to the mapReduce mas- ter. 4.7 Local Execution Debugging problems in Map or Reduce functions can be tricky, since the actual computation happens in a dis- tributed system  users have found the counter facility useful for san- ity checking the behavior of MapReduce operations. one computation searches through approxi- mately one terabyte of data looking for a particular pat- tern. the other computation sorts approximately one ter- abyte of data. the nal sorted output is written to a set of 2-way replicated GFS les. as before, the input data is split into 64MB pieces (M = 15000) the partitioning function uses the ini-tial bytes of the key to segregate it into one of R pieces. mapreduce li- brary has been used across a wide range of domains within google. it has been used across a wide range of domains within google. it has been used across a wide range of domains including: large-scale machine learning problems, clustering problems for the Google News and Froogle products, extraction of data used to produce reports of popular queries (e.g. Google Zeitgeist), extraction of properties of web pages for new exper- iments and products. the indexing process has become much easier to operate, because most of the problems caused by machine failures, slow machines, and networking hiccups are dealt with automatically by the MapRe- duce library. a key difference between these systems and MapReduce is that MapReduce exploits a restricted pro- gramming model to parallelize the user program auto- matically and to provide transparent fault-tolerance. a key difference between these systems and MapReduce is that MapReduce exploits a the restricted programming model allows us to schedule redundant executions of tasks near the end of the job. this greatly reduces completion time in the presence of non-uniformities (such as slow or stuck workers) both systems use redundant execution to recover from data loss caused by failures. unlike MapReduce, it relies on re-execution as a mechanism for implementing fault-tolerance. cluster I/O with River: Making the fast case common. Arash Baratloo, Mehmet Karaul, Zvi Kedem, and Peter Wyckoff. in Pro- ceedings of the 9th International Conference on Parallel and Distributed Computing Systems, 1996. 'result' structure contains info // about counters, time taken, number of // machines used.'result' structure contains info // about counters, time taken, number of // machines used, etc. return 0;'result' structure contains info // about counters, time taken, number of // machines used, etc.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}